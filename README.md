# Devfolio-SWOC-3.0
List Of Contributions-
1.Deep-Learning-Simplified
2.Terminal Portfolios
3.Dino
4.Docbook
5.Front-End-Projects
6.CSSdesigns-effects
7.Algorithms
....
....
....
DEEP-LEARNING-SIMPLIFIED=It includes description of Multilayer Perceptrons,Radial Basis Function Networks,Convolutional Neural Networks,Recurrent Neural Networks,Long -Short-term Memory Networks,Restricted Boltzmann Machines,Self-organising Maps,Generative Adversarial Networks,Autoencoders Deep Learning Algorithm,Deep belief Algorithm......
Deep learning is a type of machine learning and artificial intelligence (AI) that imitates the way humans gain certain types of knowledge. Deep learning is an important element of data science, which includes statistics and predictive modeling. It is extremely beneficial to data scientists who are tasked with collecting, analyzing and interpreting large amounts of data; deep learning makes this process faster and easier.

At its simplest, deep learning can be thought of as a way to automate predictive analytics. While traditional machine learning algorithms are linear, deep learning algorithms are stacked in a hierarchy of increasing complexity and abstraction.

To understand deep learning, imagine a toddler whose first word is dog. The toddler learns what a dog is -- and is not -- by pointing to objects and saying the word dog. The parent says, "Yes, that is a dog," or, "No, that is not a dog." As the toddler continues to point to objects, he becomes more aware of the features that all dogs possess. What the toddler does, without knowing it, is clarify a complex abstraction -- the concept of dog -- by building a hierarchy in which each level of abstraction is created with knowledge that was gained from the preceding layer of the hierarchy.

How deep learning works
Computer programs that use deep learning go through much the same process as the toddler learning to identify the dog. Each algorithm in the hierarchy applies a nonlinear transformation to its input and uses what it learns to create a statistical model as output. Iterations continue until the output has reached an acceptable level of accuracy. The number of processing layers through which data must pass is what inspired the label deep.

In traditional machine learning, the learning process is supervised, and the programmer has to be extremely specific when telling the computer what types of things it should be looking for to decide if an image contains a dog or does not contain a dog. This is a laborious process called feature extraction, and the computer's success rate depends entirely upon the programmer's ability to accurately define a feature set for dog. The advantage of deep learning is the program builds the feature set by itself without supervision. Unsupervised learning is not only faster, but it is usually more accurate.

Initially, the computer program might be provided with training data -- a set of images for which a human has labeled each image dog or not dog with metatags. The program uses the information it receives from the training data to create a feature set for dog and build a predictive model. In this case, the model the computer first creates might predict that anything in an image that has four legs and a tail should be labeled dog. Of course, the program is not aware of the labels four legs or tail. It will simply look for patterns of pixels in the digital data. With each iteration, the predictive model becomes more complex and more accurate.

Unlike the toddler, who will take weeks or even months to understand the concept of dog, a computer program that uses deep learning algorithms can be shown a training set and sort through millions of images, accurately identifying which images have dogs in them within a few minutes.

To achieve an acceptable level of accuracy, deep learning programs require access to immense amounts of training data and processing power, neither of which were easily available to programmers until the era of big data and cloud computing. Because deep learning programming can create complex statistical models directly from its own iterative output, it is able to create accurate predictive models from large quantities of unlabeled, unstructured data. This is important as the internet of things (IoT) continues to become more pervasive because most of the data humans and machines create is unstructured and is not labeled.

Deep learning methods
Various methods can be used to create strong deep learning models. These techniques include learning rate decay, transfer learning, training from scratch and dropout.

Learning rate decay. The learning rate is a hyperparameter -- a factor that defines the system or set conditions for its operation prior to the learning process -- that controls how much change the model experiences in response to the estimated error every time the model weights are altered. Learning rates that are too high may result in unstable training processes or the learning of a suboptimal set of weights. Learning rates that are too small may produce a lengthy training process that has the potential to get stuck.

The learning rate decay method -- also called learning rate annealing or adaptive learning rates -- is the process of adapting the learning rate to increase performance and reduce training time. The easiest and most common adaptations of learning rate during training include techniques to reduce the learning rate over time.

Transfer learning. This process involves perfecting a previously trained model; it requires an interface to the internals of a preexisting network. First, users feed the existing network new data containing previously unknown classifications. Once adjustments are made to the network, new tasks can be performed with more specific categorizing abilities. This method has the advantage of requiring much less data than others, thus reducing computation time to minutes or hours.

Training from scratch. This method requires a developer to collect a large labeled data set and configure a network architecture that can learn the features and model. This technique is especially useful for new applications, as well as applications with a large number of output categories. However, overall, it is a less common approach, as it requires inordinate amounts of data, causing training to take days or weeks.

Dropout. This method attempts to solve the problem of overfitting in networks with large amounts of parameters by randomly dropping units and their connections from the neural network during training. It has been proven that the dropout method can improve the performance of neural networks on supervised learning tasks in areas such as speech recognition, document classification and computational biology.

Deep learning neural networks?
A type of advanced machine learning algorithm, known as an artificial neural network, underpins most deep learning models. As a result, deep learning may sometimes be referred to as deep neural learning or deep neural networking.

Neural networks come in several different forms, including recurrent neural networks, convolutional neural networks, artificial neural networks and feedforward neural networks, and each has benefits for specific use cases. However, they all function in somewhat similar ways -- by feeding data in and letting the model figure out for itself whether it has made the right interpretation or decision about a given data element.

Neural networks involve a trial-and-error process, so they need massive amounts of data on which to train. It's no coincidence neural networks became popular only after most enterprises embraced big data analytics and accumulated large stores of data. Because the model's first few iterations involve somewhat educated guesses on the contents of an image or parts of speech, the data used during the training stage must be labeled so the model can see if its guess was accurate. This means, though many enterprises that use big data have large amounts of data, unstructured data is less helpful. Unstructured data can only be analyzed by a deep learning model once it has been trained and reaches an acceptable level of accuracy, but deep learning models can't train on unstructured data.

Deep learning examples
Because deep learning models process information in ways similar to the human brain, they can be applied to many tasks people do. Deep learning is currently used in most common image recognition tools, natural language processing (NLP) and speech recognition software. These tools are starting to appear in applications as diverse as self-driving cars and language translation services.

Use cases today for deep learning include all types of big data analytics applications, especially those focused on NLP, language translation, medical diagnosis, stock market trading signals, network security and image recognition.

Specific fields in which deep learning is currently being used include the following:

Customer experience (CX). Deep learning models are already being used for chatbots. And, as it continues to mature, deep learning is expected to be implemented in various businesses to improve CX and increase customer satisfaction.
Text generation. Machines are being taught the grammar and style of a piece of text and are then using this model to automatically create a completely new text matching the proper spelling, grammar and style of the original text.
Aerospace and military. Deep learning is being used to detect objects from satellites that identify areas of interest, as well as safe or unsafe zones for troops.
Industrial automation. Deep learning is improving worker safety in environments like factories and warehouses by providing services that automatically detect when a worker or object is getting too close to a machine.
Adding color. Color can be added to black-and-white photos and videos using deep learning models. In the past, this was an extremely time-consuming, manual process.
Medical research. Cancer researchers have started implementing deep learning into their practice as a way to automatically detect cancer cells.
Computer vision. Deep learning has greatly enhanced computer vision, providing computers with extreme accuracy for object detection and image classification, restoration and segmentation.
Limitations and challenges
The biggest limitation of deep learning models is they learn through observations. This means they only know what was in the data on which they trained. If a user has a small amount of data or it comes from one specific source that is not necessarily representative of the broader functional area, the models will not learn in a way that is generalizable.

The issue of biases is also a major problem for deep learning models. If a model trains on data that contains biases, the model will reproduce those biases in its predictions. This has been a vexing problem for deep learning programmers because models learn to differentiate based on subtle variations in data elements. Often, the factors it determines are important are not made explicitly clear to the programmer. This means, for example, a facial recognition model might make determinations about people's characteristics based on things like race or gender without the programmer being aware.

The learning rate can also become a major challenge to deep learning models. If the rate is too high, then the model will converge too quickly, producing a less-than-optimal solution. If the rate is too low, then the process may get stuck, and it will be even harder to reach a solution.

The hardware requirements for deep learning models can also create limitations. Multicore high-performing graphics processing units (GPUs) and other similar processing units are required to ensure improved efficiency and decreased time consumption. However, these units are expensive and use large amounts of energy. Other hardware requirements include random access memory and a hard disk drive (HDD) or RAM-based solid-state drive (SSD).

Other limitations and challenges include the following:

Deep learning requires large amounts of data. Furthermore, the more powerful and accurate models will need more parameters, which, in turn, require more data.
Once trained, deep learning models become inflexible and cannot handle multitasking. They can deliver efficient and accurate solutions but only to one specific problem. Even solving a similar problem would require retraining the system.
Any application that requires reasoning -- such as programming or applying the scientific method -- long-term planning and algorithmlike data manipulation is completely beyond what current deep learning techniques can do, even with large data.
Deep learning vs. machine learning
Deep learning is a subset of machine learning that differentiates itself through the way it solves problems. Machine learning requires a domain expert to identify most applied features. On the other hand, deep learning understands features incrementally, thus eliminating the need for domain expertise. This makes deep learning algorithms take much longer to train than machine learning algorithms, which only need a few seconds to a few hours. However, the reverse is true during testing. Deep learning algorithms take much less time to run tests than machine learning algorithms, whose test time increases along with the size of the data.

Furthermore, machine learning does not require the same costly, high-end machines and high-performing GPUs that deep learning does.

In the end, many data scientists choose traditional machine learning over deep learning due to its superior interpretability, or the ability to make sense of the solutions. Machine learning algorithms are also preferred when the data is small.

Instances where deep learning becomes preferable include situations where there is a large amount of data, a lack of domain understanding for feature introspection, or complex problems, such as speech recognition and NLP.
![image](https://user-images.githubusercontent.com/111672121/214531690-6b0da1f4-f677-448d-8610-32213f428e16.png)
![image](https://user-images.githubusercontent.com/111672121/214531960-f6d62db0-df38-4494-922c-43601047a707.png)


....
....
....
....
DOCKBOOK=
The "Fossies" Software Archive
... Fresh Open Source Software mainly for Internet, Engineering and Science ...
 Special features of the "Fossies" software archive
The "Fossies" software archive not only allows the "pure" download of software packages but also provides, among many other features, the ability to browse and search within the individual member files of the software packages in order to study for e.g. the READMEs and other documentation or to download selected individual files (e.g. images or libraries). This may avoid unnecessary file transfers and installation effort and may let you detect some otherwise undetected open source software treasures (that we name "fossies").
You may click "simply" on the download icon  in front of a package (archive) name in order to transfer that package but the "Fossies" software archive offers the following additional special features to you:

Viewing archive contents
Browsing selected archive member files (with optional code folding)
Different archive download formats
CLOC ("Count Lines of Code") analysis
Doxygen generated source code documentation
Pkgdiff generated source code and documentation changes
Software package meta information (with license files overview)
Source code misspelling reports
Browsing compressed text files
On-the-fly compression of files before transferring
Marking of new archive files
Archive-insertion-time sorted index
Alternate alphabetically sorted index
Advanced search facility
Short URLs
Brief archive file descriptions
Legacy folders
RSS feeds support
HTTPS (HTTP) support
Archives integrity and safeness
And last but not least: The term "Fresh" (original derived from "FREeware SHareware") is at the same time one of our missions so we have developed different control techniques to archive and support always the freshest releases.
INDEX Viewing archive contents (index) plus browsing archive member files
View the contents of an archive
As an special feature of the Fossies service you may click on an archive name in order to view the contents of that archive within a special page. That page gives you also the possibility to see all archive member files in different sort orders, via a menu line for e.g. like that:
   docs related | original | size (top100) | date | path | name | ext | top-path files

In the standard "docs related" view Fossies tries to assist users to get relevant package information quickly by sorting the member files into the following three categories:

   Basic infos (README, FAQ, INSTALL, ChangeLog, ...)
   Basic docs (manual pages, PDF-,HTML-,/doc/-files, ...)
   All other files (if too numereous at least 50)
The file extension sorted list (index_ax.html) offers at top an extra information line with all found extensions, like for e.g.:

   Found file extensions: 1 bat cfg dot el in jpeg py rc rst txt

If the mouse is over the word "Found" a tooltip with the number of file extensions and member files is shown, over the string "file extensions" the top 10 extensions with their number of occurrences are shown and if the mouse is over an extension a tooltip with a short explanation of the extension type is shown (using Fossies own file extension description file). Clicking on such an extension lets you jump to the first member file with that extension.

Browsing selected archive member files
As an additional - originally (Dec. 1994) probably world-wide unique - feature of the Fossies server you may click within the displayed archive index on any member name in order to browse or download that selected member.
Automatic mode (default):
The server tries to send an appropriate "Content-type" header field ("mime-type") according to the member's file extension (default: text/html). For example clicking on a GIF file (embedded in an archive file) lets see you the image via your browser or an external viewer without downloading the complete, possibly large archive file.
As a further newer special service some documentation-related files are presented more "user-friendly" respectively "readable" by trying a local pre-formatting (only in standard "automatic"-mode):

Unix/Linux manual pages
Perl POD documentation files
GNU Texinfo documentation files
Markdown, AsciiDoc and reStructuredText documentation files
and - since March 2019 - additionally (with some limitations)
Microsoft Office Open XML (DOCX) files
OpenDocument Text (ODT) files
Digital book (EPUB) files
Also pure HTML files are forced to be formatted respectively displayed in rendered format. But some HTML code for e.g. with forms or using PHP or Perl code are forced to be displayed as unformatted but syntax-highlighted source code.

Additionally most programming language, not-formatted markup language and configuration files are shown with line numbering and syntax highlighting.

All line numbers are anchored and directly addressable by adding "#l_<LINENO>" at the end of a member file URL. So for accessing for e.g. line 174 of the README file in the subdirectory foobar-x.y of package foobar-x.y.tar.gz you may just use

  https://fossies.org/linux/misc/foobar-x.y.tar.gz/foobar-x.y/README#l_174
As a new feature (Feb. 2016) Fossies source code highlighting now offers optional "code folding" for member files written in curly-bracket languages like C, C#, Go, JavaScript, Java, Lua, Perl, PHP, Ruby and others (JavaScript must be enabled). To selectively "fold" (hide and display) blocks respectively sections of source code you may click on the marked lines that begin a block. Foldable (expanded) blocks are marked by a preceding "−" character, folded (hidden respectively collapsed) blocks by a preceding "≡" character and a red marked line number and the folded block itself by a … as placeholder. Alternatively you can use the toggle buttons of the grey transparent box appearing at the bottom right to fold and unfold globally all blocks of different folding levels. See here a simple example:

   	Screenshot 1: Code unfolded	   	Screenshot 2: Code with a folded level 3 block (containing 37 lines)
   		   	
That feature may allow you to get a better overview of the overall structure of the browsed program code (as seen in your favourite source code editor).

As a further new feature (Dec. 2016) Fossies allows now to select the highlight(ing) style (theme). Out of nearly a hundred different styles you can choose via a tooltip box or a style selection page (with style previews) two different styles (saved by a cookie): One for programming language files and another for plain text files (if both styles are "standard" the according cookie is removed).

Text (raw) mode:
In order to view the "raw" source code file you may force a "t(ext)"-mode transfer ("Content-type: text/plain") by adding the parameter "m=t" to the requested member URL, for e.g.
  https://fossies.org/linux/misc/foobar-x.y.tar.gz/foobar-x.y/README.md?m=t
Remark: For non-text "binary" files (like for e.g. images, videos or executables) the "text mode" is principally meaningless so Fossies tries instead to give out some hopefully interesting meta information about the requested member file!
Binary (download) mode:
In order to force a binary transfer ("Content-type: application/octet-stream") for e.g. to save the file you may click on the member's "file size" field instead of the member's "file name". Or you can use the "b(inary)"-mode directly (adding the parameter "m=b") by calling for e.g.
  https://fossies.org/linux/misc/foobar-x.y.tar.gz/foobar-x.y/README.md?m=b
Remark: This feature uses sometimes on-the-fly expansion so displaying the requested member may take some seconds according to the archive size. But repeated requests are answered immediately.
Media type (download) mode:
If you prefer for some reasons to download a file with the appropriate media (MIME) type you can add the parameter "m=m"
  https://fossies.org/linux/misc/foobar-x.y.tar.gz/foobar-x.y/README.md?m=m
Caution: This may force your browser to call an assigned application.
Different archive download formats
In that page you can see all the member files of an archive in different listing orders, and browse/download that members. That page allows also the archive download in different archive formats respectively newer not yet common but more effective compression types:
tar.gz
tar.bz2
tar.xz
zip
So you can choose an compression format suited for your environment.
If you move the mouse over the filename of such an alternative compressed archive its size in bytes is shown and also the percentual size decrease compared to the "standard" format. Since March 2017 the "standard" format is no longer only tar.gz or zip but the highest compressed tar file (tar.xz, tar.bz2 or tar.gz) originally available or zip.

To download an archive in a different supported format you may change in a Fossies standard download URL simply the archive file extension, so instead of

 https://fossies.org/linux/misc/foobar-x.y.tar.gz
you may use
 https://fossies.org/linux/misc/foobar-x.y.tar.xz
for e.g. to force a faster download by a higher compression.
CLOC ("Count Lines of Code") analysis
As a further special feature you can view a so-called CLOC ("Count Lines of Code") statistics that counts physical lines of source code in many programming languages and in addition comments and blank lines, so you can get quickly an overview about the scale of the project and the used program languages.
Hint (July 2014): Files that are contained within the project packages but are probably generated by "autotools" programs are now excluded since they "distort" in some respects the CLOC analysis.

Hint (June 2020): Additionally to the existing "standard" CLOC analysis (used filename cloc.html) there is now done an alternative CLOC analysis named cloc_cs.html (a.o. better suited for an optional codespell check rating). Perhaps the results are more "realistic" since it tries to exclude third party code but also files containing fonts, codepage or character set definitions, dictionaries, names, SVG or non-English languages.

Doxygen generated source code documentation
As an advanced feature - especially useful for developers - Fossies offers Doxygen generated source code documentation for most archives respectively software packages (cross referencing documentation and code). Although Doxygen isn't package specific customized here we hope that it would be nevertheless helpful for understanding code structures and detecting dependencies - especially for large projects. And it may encourage authors or package maintainers to create her own specific and thereby probably improved Doxygen documentation.
Doxygen generates amongst others a class browser and tries to extract the code-structure also from undocumented source files. This includes dependency graphs (see an example here), class diagrams and hyperlinked syntax-highlighted source code (latter as an alternative to the origin Fossies offered source code browsing using the program "highlight").

Doxygen documentation is extracted directly from the sources, which makes it much easier to keep the documentation consistent with the source code.

The Fossies generated Doxygen documentation can be accessed via the respective package contents pages or more directly via URLs like

  https://fossies.org/dox/foobar-x.y/...
Also version-independent URLs are usable. If the specified package exists they are redirected to the documentation of the current version:
  https://fossies.org/dox/foobar/...
So for e.g. accessing the main page of the doxygen documentation of the package "pcre" you may use just
  https://fossies.org/dox/pcre/
For more information see the Fossies Dox page.
Pkgdiff generated source code and documentation changes
With the beginning of the year 2015 the "Fossies" software archive offers a further new service for most of the provided open source software packages on Fossies: "pkgdiff"-generated reports of source code changes between the current and the preceding version.
This extension may be of particular interest to software authors and developers but also to package maintainers and installers. The service appears to be especially useful for software projects that are not hosted on big code management or repository systems with similar or even more powerful services.

The Fossies generated so-called "diffs"-reports can be accessed via the respective package contents pages or more directly via URLs like

  https://fossies.org/diffs/...
For more information see the Fossies Diffs page.
Software package meta information (with license files overview)
Since end of March 2022 the Fossies software archive offers for all source code packages a "meta" information page. This page collects some package information mostly also available elsewhere on Fossies like a short description, the home page, byte size, modification date and md5 checksum, number of package menbers, and a list of found file extensions with direct access to the according member files. But as a special feature it contains a list of all found license-related files with the attempt to classify them as internal or external ones. Additionally the result of a license type analysis is shown together with some extracted copyright information. If no type could be determined a short hopefully relevant text extract is displayed.
But naturally the offered lists of licences are unofficial and they may be even incomplete. It is just an attempt to provide a first related overview. Although detailed license conditions can be found in the according linked text files and the named license information pages, the user should study the project itself for the relevant licenses.

The Fossies generated so-called "meta"-reports can be accessed like the above described services via the "Fossies services:" line in the respective package contents pages via URLs like

  https://fossies.org/<folder>/<package>/meta.html
  https://fossies.org/<folder>/<package>/meta.html#l[ic[ense]]  (to jump directly to the license information)
or more generally via URLs like
  https://fossies.org/meta/<package>
  https://fossies.org/meta/<os>/<package>
  https://fossies.org/meta/<folder>/<package>
If the given package or project information isn't unique an according selection page is shown.
As an alternative and easier to use method (e.g. if the URL is also available in the browser address bar) one can add just the parameter "?M" after a package related URL (but not after a package member related URLs). That leads to a redirect to the according "meta" page.

Source code misspelling reports
Although misspelling corrections are not the most exciting issues - and the spelling errors are rarely true code bugs but mostly contained in the comment and documentation parts - they can still improve the overall quality of a software project a little bit.
So since October 2019 Fossies offers optionally source code misspelling check reports using as basis the command line tool "codespell". The resulting analysis reports are currently per default not linked, on the one hand since they are essentially only of interest to the authors itself, on the other hand to avoid a taste of know-all-manner and snooping around. But if available they will be visible by directly accessing URLs like

  https://fossies.org/<folder>/<package>/codespell.html
The codespell analysis reports show the top found spelling error types and a sortable list of all found errors together with the suggested fixes, the error context type (indicated by a single character), the affected member files and the according line numbers. Clicking on a red colored spelling error in the "top" list jumps to the first match in the actual spelling error occurrence list. Clicking on a matching member file lets you browse that file while jumping to the according red marked line (in some cases the matching line may lie in the neighborhood).

Mainly for "big" software packages the list may contain also wrong matches (so-called FPs). Especially variable names, mail addresses and words that directly "touch" some kinds of delimiters or escape characters may be problematic and for some words the correct spelling is even ambiguous. Although after a thorough review done by Fossies most of them should be filtered out (ignored) don't hesitate to send such words (FPs) to the mail address below so that a new improved check can be done if applicable.

More as a gimmick than as a serious rating Fossies assigns to every analyzed package a spelling error grade ranging from "A+" (excellent) til "F" (unsatisfactory) in the hope that motivates the authors to correct the found spelling errors and to report the false positives. And yes, even according codespell report badges are available.

A note: Fossies provides as an information only the source code spelling check reports with the special option to inspect the context of the probably misspelled words in a fast and comfortable way. However Fossies doesn't create for e.g. "diff"-like files usable by the "patch" command or even GitHub pull requests (PRs) since the risk is too high that something would be wrongly corrected (but here you may find some small codespell usage hints). Whether it's worth the effort to correct the misspelling errors manually is a decision of the respective project.

If you - especially as an author or a maintainer - are nevertheless interested in such an analysis report please send us an according email to info@fossies.org.

Such reports can be requested only once for the current release or generally for the current and all future releases and even additionally or alternatively for corresponding development versions (maybe as part of a CI process). In the latter case according tarballs will be built from cloned repositories (e.g. "master" from GitHub) and put into a special folder named "test" that isn't fully integrated into the Fossies services so the results are not visible for normal users and denied for search engines. On request notification e-mails could be sent if new spelling errors are detected.

An optionally requestable corresponding link would be named "Codespell check" and added to the "Fossies services" line at top of the package contents pages to the other three main special Fossies service links as shown here (this example has only tooltips but no real links):

   Fossies services: Doxygen documentation | Diffs report | Codespell check | CLOC analysis | Meta information

A typical report can be seen here.

UNCOMPRESS Browsing compressed text files
You may click on the icons placed in front of compressed text files of the following type
.Z
.gz
to browse that files forcing on-the-fly expansion.
FILE On-the-fly compression of files before transfer
You may click on one of the optional
  (Z/gz/zip)
strings after the file description of uncompressed files in order to transfer it as a
  Z   = "Unix-compressed" 
  gz  = "GNU-zipped"
  zip = "(PK-)zipped"
file in order to save network bandwidth.
*** NEW ***Marking of new archive files
Files that are recently inserted in the software archive are marked by two small icons:
*** NEW *** file is not older than 7 days
*new* file is not older than 31 days

[new...] button Archive-insertion-time sorted index
Standardally the archive index is sorted by the archive-insertion-time (not the modified time) in order to get in a clearly arranged matter information about recently archived software.
[abc...] button Alternate alphabetically sorted index
Beside the "standard" archive-insertion-time sorted archive index you may select an alternate alphabetically sorted index.
[search] button Advanced search facility
In order to find a given software package or file you may search recursively starting at the current software folder. The optional use of different approximative search modes and the optional inclusion of file description texts into the search space may let you find a match even if you don't remember a filename exactly (e.g. "Firefoks" instead of "firefox") or if you search for a more generic term (e.g. "Web browser").
You may specify the

search space:
package filenames
package description texts
package internal member files (resp. filenames) *NEW*
contents of package internal documentation files
search mode:
exact
max. one or two allowed errors
"best guess"
search depth (search in underlying folders)
case sensitivity
Within the particular package contents pages now also package specific searches are possible: Either for package member names or - probably more interesting - full text searches for contents of all package member text-like files (thus not only searching the documentation files but also for e.g. all source code files itself). As a small exemplification you may find here a package contents search use case illustrated by some screenshots.

If multiple search space fields are filled the "lowest" one is used with the following exception: The topmost "file names" field may be used to restrict the search space for the other searches.

Searching in the contents of the package internal documentation (or source code) files outputs not only the found files but also a user specified number of matching lines. Browsing a found file displays at the top of the file the first matching lines (default max. 10) with anchors into the file and tries to mark the matches as red-bolded strings.

The total number of displayed matches may be limited. Consecutive requests for the display of suppressed matches - with optionally different limits - are supported.

Attention: The search-syntax is currently NOT "google"-like but supports simple regular expressions. So be aware that some characters like

  ^ $ . ? * + [ ] { } ( ) \
have a special meaning.
So if you want to use any of these characters as a literal you need to escape it with a backslash: For e.g. if searching for something containing the string "gtk+-3" your search input string must be "gtk\+-3" otherwise you would search due to the special meaning of the "+" in regular expressions for "gtk-3", "gtkk-3", "gtkkk-3" with an arbitrary number of consecutive "k" characters.

Here are some simple general examples:

foo bar
would search for a line containing the exact string "foo bar"
foo.*bar
would search for a line containing first "foo" and than "bar" for e.g. "foo and bar" (regular expression)
^foo
would search for a line starting with "foo" but wouldn't match a line starting for e.g. with "The foo" (regular expression)
bar$
would search for a line ending with "bar" but wouldn't match a line ending for e.g. with "bars" (regular expression)
The search string must contain at least four "relevant" characters if searching within the archives internal doc files, three if searching within the archive short descriptions and two if searching for filenames.

Short URLs
To make the finding and download of packages more comfortable and easier Fossies supports also short URLs that are independent of the package version and Fossies folder path. They have just the simple form
  https://fossies.org/<package>/
For e.g. the URL
  https://fossies.org/foobar/
may redirect you automatically to
  https://fossies.org/linux/misc/foobar-x.y.tar.gz/
If the redirect isn't unique Fossies will show you all matches (including the according Fossies folder paths) and you can choose yourself the appropriate link!
The string "<package>" in the short URL may be for e.g. a name like "atool" or "glpk" (with probably unique matches so direct redirects) or "gcc" or "ImageMagick" (with probably multiple matches so redirects to a selection page).

Similar package version independent short URLs exist for the Doxygen-generated source code documentation:

  https://fossies.org/dox/<package>/
Archive file descriptions
All packages contain besides a link to the original home page (if known and available) additionally a brief description to make it easier for the user to evaluate the type and intended use of a given archive file respectively software package. In a second step the user can take advantage of the special Fossies browsing features and study available READMEs, FAQ or other appropriate internal documentation like the manual pages.
Legacy folders
Every main Linux folder has a sub-folder named "legacy". That folders aren't intended for normal usage but are just a collection of previous (legacy) releases that were in the past available in the according parent main folder but are now replaced on Fossies by newer production releases. The packages in the legacy folders are offered without any freshness maintenance only as a kind of long-term availability support service for some Fossies referencing external sites. An internal mapping system ensures that the "legacy" packages remain accessible via its old (original) URLs to avoid stale links on external sites. Normally at least the last "legacy" package of a project is kept for 2 months but noteworthy accessed ones even for 6 months.
RSS feeds support
To be always promptly informed about the newest software packages on Fossies you may subscribe to one of the different Fossies RSS feeds.
HTTPS (HTTP) support
Since September 2012 Fossies is also accessible and since October 20, 2016 only accessible via the HTTPS protocol to add the security capabilities of SSL/TLS to standard HTTP communications. For a limited period the Fossies resources are still available via HTTP by using the server name fresh-center.net:
    http://fresh-center.net/

Archives integrity and safeness
To ensure the integrity of the archive files offered by Fossies they are downloaded almost invariably from the original site respectively home page or its official mirrors.
Of course Fossies also attempts to offer only archives that are virus and malware free. Since Februar 2017 this will be mainly achieved via external checks by the VirusTotal site that let scan the files by multiple well-known virus/malware detecting engines. The results are visible at the top of the particular archive contents page, normally as a green "Ok", but according to the number of matching scanners (N) also like

  (pending) 	  VirusTotal check: Sorry, no results yet (but expected shortly)
  for N=0  	  VirusTotal check: Ok
  for N=1  	  VirusTotal check: Very probably ok    
  for N=2  	  VirusTotal check: Probably ok
  for N=3-4  	  VirusTotal check: Maybe ok (but 3 of 56 scan engines found a match)
  for N=5-6  	  VirusTotal check: Initial suspicion (5 of 56 scan engines found a match)
   ... and probably removed (or to be removed) on Fossies:
  for N=7-8  	  VirusTotal check: Sneaking suspicion (7 of 56 scan engines found a match)
  for N=9-12  	  VirusTotal check: Suspicion (9 of 56 scan engines found a match)
  for N=13-16  	  VirusTotal check: Real suspicion (13 of 56 scan engines found a match)
  for N≥17  	  VirusTotal check: Strong suspicion (17 of 56 scan engines found a match)
The detailed VirusTotal report can be seen by clicking on the according Fossies short valuation text. Suspicious archives (for e.g. with more than 6 matching engines of typically about 50 scanning engines) should be removed on Fossies.

In addition most of the external links on Fossies (mainly to the archives home pages within the folder contents pages but also links found in the main archive documentation files) are periodically checked against the Google Safe Browsing blacklist.

After a download from Fossies itself you can check the archive integrity by the checksums given at the bottom of the archive contents pages (MD5, SHA1 and SHA256) or by downloading (also for the alternative download formats) via

  https://fossies.org/.../<package>_fossies.<checksum>
   or (shorter but less specific)
  https://fossies.org/.../<package>.<checksum>
with the <checksum> string equal md5, sha1 or sha256, for e.g.
  https://fossies.org/linux/misc/foobar-x.y.tar.xz_fossies.sha256
   or
  https://fossies.org/linux/misc/foobar-x.y.tar.gz.md5
Hint: The checksums are generated on Fossies and are not the original ones that may be exist on the project pages itself (but naturally they should be identical).

The above descripted features are enabled by software written by Jens Schleusener mainly in his spare time - while employed at the Central Division Data Processing of the German Aerospace Center (DLR) which division was later incorporated in the T-Systems Solutions for Research GmbH (SfR). That software consists mainly of two core components:

warix (Www ARchive IndeXer)
the preprocessing script generating the "static" index pages and the internal databases
warex (Www ARchive EXecutor)
the accompanying CGI-script processing the "dynamic" output requests.
Furthermore the Fossies software archive itself benefits awfully from many great open source software products. Especially to be mentioned:

Doxygen
Pkgdiff
Highlight
Pandoc
CLOC
Codespell
and for the basics
Apache HTTP Server
ModSecurity (CRS)
Use of the Fossies software archive is entirely at your own risk - no warranty is expressed or implied.
 ....
 ....
 ....
 ....
 ![image](https://user-images.githubusercontent.com/111672121/214532409-e97ed791-5ed5-4bd4-aad1-7ae4466f2245.png)

 TERMINAL PORTFOLIO=Terminal Style Portfolio made with HTML / CSS / JS
 ![image](https://user-images.githubusercontent.com/111672121/214532925-27760a38-2ee7-48ba-88c9-1caf80f292b4.png)
![image](https://user-images.githubusercontent.com/111672121/214533085-fb1c486f-49c2-4b4a-a9ee-8095a65620a7.png)
![image](https://user-images.githubusercontent.com/111672121/214533245-e09553d3-93c9-4fca-a39c-0977427784d4.png)



